{"cells":[{"cell_type":"code","source":["## Spark Libraries\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.classification import LogisticRegression ## Logistic regression operations\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator ## create an evaluator\nfrom pyspark import SparkContext ## Creating spark context\nfrom pyspark.mllib.util import MLUtils ## Load,save ,preprocess the data in ml library\nfrom pyspark.ml.feature import VectorAssembler ## convert features columns into vector \nimport pandas as pd #for dataframe related operations\nfrom pyspark.sql import Row ## New rows can be created\nfrom pyspark.ml.feature import StandardScaler ## Scaling data of different metrics into a common scale\nfrom pyspark.sql.functions import when ##for ifelse statement for preprocessing data\nfrom pyspark.ml import Pipeline, PipelineModel, feature, regression, classification, evaluation ## all ml related \nfrom pyspark.ml.classification import DecisionTreeClassifier## decision tree operations\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics ##evaluation performance of the model by auc score \nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics as metrics \nfrom sklearn.metrics import roc_curve, auc, classification_report # evaluation metrics\nfrom matplotlib import pyplot as plt ## Plotting operations"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#Creating spark session \nspark"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#Reading data from data source in databricks\ndf= spark.read.table('credit_card_csv')"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#display imported data\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["df = df.drop('_c0')\ndf = df.drop('?ID')"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["df=df.withColumn('SEX', when(df.SEX==1, 0).otherwise(1)) "],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["df=df.withColumn('MARRIAGE', when(df.MARRIAGE==1, 0).otherwise(1))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#displaying refined data\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["## Splitting Data Set\nsplit = df.randomSplit([0.6,0.3,0.1]) #Defining split ratios train = 60%, validation= 30%, test = 10%\ntrain = split[0]\nvalidation= split[1]\ntest = split[2]"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["x = train.count()\nz = validation.count()\ny = test.count()\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["## Creating Dataframe  for training, validation and testing data \nl = [('Training',x),('Validation',z),('Testing',y)] ##put values for dataframe into list\nrdd = sc.parallelize(l)\ndatasplit = rdd.map(lambda x: Row(DataType=x[0], Size=int(x[1])))\ndatasplt = sqlContext.createDataFrame(datasplit)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["datasplt.show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["datasplt.show()\ndisplay(datasplt)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["display(train)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["assembler1 = VectorAssembler(inputCols=[\"LIMIT_BAL\",\"AGE\",\"BILL_AMT1\",\"BILL_AMT2\",\"BILL_AMT3\",\"BILL_AMT4\",\"BILL_AMT5\",\"BILL_AMT6\",\"PAY_AMT1\",\"PAY_AMT2\",\"PAY_AMT3\",\"PAY_AMT4\",\"PAY_AMT5\",\"PAY_AMT6\"],outputCol = 'scalingfeatures')"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["scaler_1 = StandardScaler(inputCol=\"scalingfeatures\",outputCol=\"scaledFeatures\",withStd=True, withMean=False)\nscaled_training = assembler1.transform(df)\nscalerModel = scaler_1.fit(scaled_training)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["assembler2 = VectorAssembler(inputCols=[\"SEX\",\"EDUCATION\",\"MARRIAGE\",\"PAY_0\",\"PAY_2\",\"PAY_3\",\"PAY_4\",\"PAY_5\",\"PAY_6\",\"scaledFeatures\"],outputCol = 'features')"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["lr_2 = LogisticRegression(featuresCol='features', labelCol='default payment next month')\npipe_lr= Pipeline(stages=[assembler1,scaler_1,assembler2,lr_2])\npipe_model= pipe_lr.fit(train)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["predictions_pipe = pipe_model.transform(validation)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["print(\"Binomial coefficients: \" + str(pipe_model.stages[3].coefficientMatrix))\nprint(\"Binomial intercepts: \" + str(pipe_model.stages[3].interceptVector))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["coeff=[stage.coefficients for stage in pipe_model.stages if hasattr(stage, \"coefficients\")]\ncoeff = pd.DataFrame(coeff)\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["from pyspark.sql import SQLContext"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["sqlContext = SQLContext(sc)\nsqlCxt = sqlContext.createDataFrame(coeff)\ndisplay(sqlCxt)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["display(predictions_pipe)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\nbce = BinaryClassificationEvaluator()\nbce.setLabelCol('default payment next month')\nauclr = bce.evaluate(pipe_model.transform(validation))\nauclr"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["predicted = predictions_pipe.select(\"features\",\"prediction\",\"default payment next month\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["display(predicted)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["## Defining Accuracy Evaluator\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"default payment next month\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions_pipe)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["accu1 = accuracy*100\nprint accu1"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["\nreg_param = 0.02\nen_param = 0.3\n\nlr_enr = classification.LogisticRegression(labelCol= 'default payment next month' , featuresCol=\"features\").setRegParam(reg_param).\\\n       setElasticNetParam(en_param)\npipe_enr= Pipeline(stages=[assembler1,scaler_1,assembler2,lr_enr])\nlr_2model  = pipe_enr.fit(train)\n"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["print(\"Binomial coefficients: \" + str(lr_2model.stages[3].coefficientMatrix))\nprint(\"Binomial intercepts: \" + str(lr_2model.stages[3].interceptVector))"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["from pyspark.sql import SQLContext"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["sqlContext = SQLContext(sc)\nsqlCxt = sqlContext.createDataFrame(coeff)\ndisplay(sqlCxt)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["## Predictions based on Elastic Net Regression\nprediction_enr = lr_2model.transform(validation)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\nbce = BinaryClassificationEvaluator()\nbce.setLabelCol('default payment next month')\naucENR = bce.evaluate(lr_2model.transform(validation))\naucENR"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["dt = DecisionTreeClassifier(labelCol=\"default payment next month\", featuresCol=\"features\")\npipe_dt= Pipeline(stages=[assembler1,scaler_1,assembler2,dt])"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["model1 = pipe_dt.fit(train)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["treeModel = model1.stages[3]"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["predictions2 = model1.transform(validation)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\nbce = BinaryClassificationEvaluator()\nbce.setLabelCol('default payment next month')\naucDT = bce.evaluate(model1.transform(validation))\naucDT"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["display(predictions2)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics as metrics\nresults = predictions2.select(['probability','default payment next month'])\nresults_collected = results.collect()\nresults_list=[(float(i[0][0]),1.0-float(i[1])) for i in results_collected]\nscoresandlabels = sc.parallelize(results_list)\n\nmetrics = metrics(scoresandlabels)\naucDT = metrics.areaUnderROC\nprint(metrics.areaUnderROC)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["predictions2.select(\"prediction\", \"default payment next month\", \"features\").show(5)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["accuracy2 = evaluator.evaluate(predictions2)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["aacu2 = accuracy2*100\nprint aacu2"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n\nrf = RandomForestClassifier(labelCol=\"default payment next month\", featuresCol=\"features\", numTrees=10)\npipe_rf= Pipeline(stages=[assembler1,scaler_1,assembler2,rf])"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["model2 = pipe_rf.fit(train)\npredictions3 = model2.transform(validation)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\nbce = BinaryClassificationEvaluator()\nbce.setLabelCol('default payment next month')\naucRF = bce.evaluate(model2.transform(validation))\naucRF"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["predictions3.select(\"features\",\"default payment next month\",\"prediction\").show(5)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["importances = model2.stages[3].featureImportances\nimportances"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["accuracy3 = evaluator.evaluate(predictions3)\naccu3 = accuracy3*100\nprint (\"accuracy of Random Forest : \", accu3,\"%\")"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["from pyspark.ml.classification import GBTClassifier\ngbt = GBTClassifier(labelCol=\"default payment next month\", featuresCol=\"features\", maxIter=10)\npipe_gbt= Pipeline(stages=[assembler1,scaler_1,assembler2,gbt])\n\nmodel4 = pipe_gbt.fit(train)\n\npredictions4 = model4.transform(validation)\n\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nbce = BinaryClassificationEvaluator()\nbce.setLabelCol('default payment next month')\naucGBM = bce.evaluate(model4.transform(validation))\naucGBM\n"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["predictions4.select(\"prediction\", \"default payment next month\", \"features\").show(5)\n\naccuracy4 = evaluator.evaluate(predictions4)\n\naccu4 = accuracy4*100\nprint(\"Accuracy of Gradient boostin\",accu4,\"%\")"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["## Calculating Auc score for GBM model\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics as metrics\nresults = predictions4.select(['probability','default payment next month'])\nresults_collected = results.collect()\nresults_list=[(float(i[0][0]),1.0-float(i[1])) for i in results_collected]\nscoresandlabels = sc.parallelize(results_list)\n\nmetrics = metrics(scoresandlabels)\naucGBM = metrics.areaUnderROC\nprint(metrics.areaUnderROC)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["##Creating Dataframe for visualization \nfrom pyspark.sql import Row\nfrom pyspark.sql import SQLContext\nl = [('Logistic Regression',auclr*100),('Elastic Net',aucENR*100),('Decision Tree',aucDT*100),('Random Forest',aucRF*100),('Gradient Boosting',aucGBM*100)]\nrdd = sc.parallelize(l)\nperformance = rdd.map(lambda x: Row(Model=x[0], Auc_Score=int(x[1])))\nmodelperformance = sqlContext.createDataFrame(performance)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["modelperformance.show()"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["display(modelperformance)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["## Best Model is Random Forest and Gradient boosting\n## Running Random Forest on Test Data\npredictions_test = model2.transform(test)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["accuracy_final= evaluator.evaluate(predictions_test)\naccuracy_final"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\nbce = BinaryClassificationEvaluator()\nbce.setLabelCol('default payment next month')\nauc_final = bce.evaluate(model2.transform(validation))\nauc_final"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["### Calculating AUC score and plotting ROC curve\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n \ny_test1 = [i[1] for i in results_list]\ny_score1 = [i[0] for i in results_list]\n \nfpr, tpr, threshold = roc_curve(y_test1, y_score1)\nroc_auc = auc(fpr, tpr)\n \n%matplotlib inline\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()\n\ndisplay()"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["## input values\nsex= 0\nmarriage= 0\nPay_0=-1\nPay_2=2\nPay_3=2\nPay_4=4\nPay_5=3\nPay_6=2\nLimit_Bal=100000\nAge=46\nBill_AMT1=6277\nPAY_AMT1=1000\n\nfrom pyspark.sql import Row\nfrom pyspark.sql import SQLContext\nl = [(sex,marriage,Pay_0,Pay_2,Pay_3,Pay_4,Pay_5,Pay_6,Limit_Bal,Age,Bill_AMT1,PAY_AMT1)]\nrdd = sc.parallelize(l)\ntesting_performance = rdd.map(lambda x: Row(SEX=x[0], MARRIAGE=int(x[1]),PAY_0=int(x[2]),PAY_2=int(x[3]),PAY_3=int(x[4]),PAY_4=int(x[5]),PAY_5=int(x[6]),PAY_6=int(x[7]),LIMIT_BAL=int(x[8]), AGE=int(x[9]),BILL_AMT1=int(x[10]),PAY_AMT1=int(x[11])))\ntestingperformance = sqlContext.createDataFrame(testing_performance)"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["testingperformance.show()"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["## model building\nassembler_test = VectorAssembler(inputCols=[\"LIMIT_BAL\",\"AGE\",\"BILL_AMT1\",\"PAY_AMT1\"],outputCol = 'scalingfeatures')"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["scaler_test = StandardScaler(inputCol=\"scalingfeatures\",outputCol=\"scaledFeatures\",withStd=True, withMean=False)\nscaled_training = assembler_test.transform(df)\nscaler_testModel = scaler_test.fit(scaled_training)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["assembler2_test = VectorAssembler(inputCols=[\"SEX\",\"MARRIAGE\",\"PAY_0\",\"PAY_2\",\"PAY_3\",\"PAY_4\",\"PAY_5\",\"PAY_6\",\"scaledFeatures\"],outputCol = 'features')"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["reg_param=0.02\nen_param=0.3\n\nlr_enr_test = classification.LogisticRegression(labelCol= 'default payment next month' , featuresCol=\"features\").setRegParam(reg_param).\\\n       setElasticNetParam(en_param)\npipe_enr_test= Pipeline(stages=[assembler_test,scaler_test,assembler2_test,lr_enr_test])\nlr_2model_test = pipe_enr_test.fit(train)"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["lr_2model_test.stages[3].coefficientMatrix"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["\ntest_prediction = lr_2model_test.transform(testingperformance)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["test_prediction.select(\"probability\",\"prediction\").show()"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":72}],"metadata":{"name":"IST 718 Credit defaulter prediction","notebookId":4329625624595310},"nbformat":4,"nbformat_minor":0}
